{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8VcMvpsTdi8"
      },
      "source": [
        "## BSS Identification Using Machine Learning\n",
        "\n",
        "Yuvraj Sahu, Andrew Harvey, Elijah Flores\n",
        "\n",
        "The University of Texas at Austin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mK4_cks9Z0p0"
      },
      "outputs": [],
      "source": [
        "# Import statements\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mBFjdCnp-Psj"
      },
      "outputs": [],
      "source": [
        "# Global parameters used for running the experiments - note that these can be\n",
        "# changed and tuned for running different experiments\n",
        "g_data                 = pd.read_csv('LiStars.csv')\n",
        "g_features             = ['Gmag', 'BPmag', 'RPmag', 'Gmagcor', '(G-RP)cor']\n",
        "g_models               = [DecisionTreeClassifier, SVC, LogisticRegression]\n",
        "g_result_column        = 'Note'\n",
        "g_result_nss_indicator = 'N'\n",
        "g_result_bss_indicator = 'BS'\n",
        "g_num_iterations       = 500\n",
        "g_initial_seed         = 737_132"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PosAG2L2sHb7"
      },
      "outputs": [],
      "source": [
        "# Translate an integer representing the combination ID (comb_id) into a list of\n",
        "# features based on all of the available features\n",
        "def translate_comb(features, comb_id):\n",
        "    selected_features = []\n",
        "    for i in range(len(features)):\n",
        "        if (comb_id & (1 << i)) == 0:\n",
        "            selected_features.append(features[i])\n",
        "    return selected_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ULh7iUK1TDX_"
      },
      "outputs": [],
      "source": [
        "# Retrieves the combination ID given the feature list an a subset of selected\n",
        "# features; note that this function is not used in this program, but is meant\n",
        "# to allow other researchers to more easily tailor this program to their needs\n",
        "def get_comb_id(features, selected_features):\n",
        "    inverse_id = 0\n",
        "    for selected_feature in selected_features:\n",
        "        inverse_id |= 1 << features.index(selected_feature)\n",
        "    all_ones = (1 << len(features)) - 1\n",
        "    return inverse_id ^ all_ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Cn-U_nrd15-t"
      },
      "outputs": [],
      "source": [
        "# Selects a random subset of the data that contains at most new_size rows\n",
        "def sample_data(data, new_size):\n",
        "    data_size = data.shape[0]\n",
        "    if new_size < data_size:\n",
        "        sample = np.random.choice(data_size, size = new_size, replace = False)\n",
        "        data = data[sample]\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ghy1PTOOh7ns"
      },
      "outputs": [],
      "source": [
        "# Generates a test case (training and testing data for x and y) based on the\n",
        "# subset of NSS and BSS parameters currently in use; returns the 4-tuple\n",
        "# (x_train, x_test, y_train, y_test)\n",
        "def generate_testcase(nss_sub, bss_sub):\n",
        "    size = min(nss_sub.shape[0], bss_sub.shape[0])\n",
        "    nss_sample = sample_data(nss_sub, size)\n",
        "    bss_sample = sample_data(bss_sub, size)\n",
        "    x = np.concatenate((nss_sample, bss_sample))\n",
        "    y = np.concatenate((np.zeros(size, dtype=int), np.ones(size, dtype=int)))\n",
        "    testcase = train_test_split(x, y, test_size = 0.3)\n",
        "    return testcase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mj5n1itsrAtp"
      },
      "outputs": [],
      "source": [
        "# Creates an instance of the given model and runs the test case on it,\n",
        "# returning the accuracy achieved\n",
        "def run_instance(model, testcase):\n",
        "    x_train, x_test, y_train, y_test = testcase\n",
        "    model_instance = model()\n",
        "    model_instance.fit(x_train, y_train)\n",
        "    y_pred = model_instance.predict(x_test)\n",
        "    accuracy = np.sum(y_test == y_pred) / y_pred.size\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6fvv4pMNyw6k"
      },
      "outputs": [],
      "source": [
        "# Goes through each combination of features (except the empty set) and tests\n",
        "# each model for the given number of iterations; returns a dictionary mapping\n",
        "# model names to performance, where performance is represented as a list of\n",
        "# accuracies indexed by combination ID\n",
        "def evaluate_models(nss, bss, features, models, num_iterations):\n",
        "    num_combs = (1 << len(features)) - 1\n",
        "    output = {model.__name__: [0.0] * num_combs for model in models}\n",
        "    for comb_id in range(num_combs):\n",
        "        selected_features = translate_comb(features, comb_id)\n",
        "        nss_sub = nss[selected_features].to_numpy()\n",
        "        bss_sub = bss[selected_features].to_numpy()\n",
        "        for iteration in range(num_iterations):\n",
        "            testcase = generate_testcase(nss_sub, bss_sub)\n",
        "            for model in models:\n",
        "                accuracy = run_instance(model, testcase)\n",
        "                output[model.__name__][comb_id] += accuracy / num_iterations\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HoA6aDiYzvKy"
      },
      "outputs": [],
      "source": [
        "# Formats and prints the results outputted from the evaluate_models function\n",
        "def print_results(results, features):\n",
        "    num_combs = (1 << len(features)) - 1\n",
        "    feature_text_len = sum(map(len, features)) + 2 * len(features)\n",
        "    formatted_features = [\n",
        "        ', '.join(translate_comb(features, comb_id)).ljust(feature_text_len)\n",
        "        for comb_id in range(num_combs)\n",
        "    ]\n",
        "\n",
        "    for model_name, accuracies in results.items():\n",
        "        print(f'### {model_name} Results ###')\n",
        "        iterator = sorted(range(num_combs), key = lambda i: -accuracies[i])\n",
        "        for i in iterator:\n",
        "            formatted_accuracy_percent = format(accuracies[i] * 100, '.3f')\n",
        "            print(f'{formatted_features[i]}{formatted_accuracy_percent}%')\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0C6KLH--b6yq"
      },
      "outputs": [],
      "source": [
        "# Sets a random seed (for reproducibility), finds the NSS and BSS data, and\n",
        "# runs the experiments\n",
        "def run_all_tests(data, features, models, result_column, result_nss_indicator,\n",
        "                  result_bss_indicator, num_iterations, initial_seed):\n",
        "    np.random.seed(initial_seed)\n",
        "    result = data[result_column]\n",
        "    nss = data[result == result_nss_indicator]\n",
        "    bss = data[result == result_bss_indicator]\n",
        "    output = evaluate_models(nss, bss, features, models, num_iterations)\n",
        "    print_results(output, features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtSPfKtN_Ud2",
        "outputId": "4b835e8f-023e-4f35-e5d7-c41dbdd4eb73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### DecisionTreeClassifier Results ###\n",
            "Gmagcor, (G-RP)cor                      76.731%\n",
            "BPmag, (G-RP)cor                        76.065%\n",
            "Gmag, BPmag, Gmagcor, (G-RP)cor         75.955%\n",
            "BPmag, RPmag, (G-RP)cor                 75.873%\n",
            "RPmag, Gmagcor, (G-RP)cor               75.841%\n",
            "BPmag, Gmagcor, (G-RP)cor               75.816%\n",
            "Gmag, RPmag, Gmagcor, (G-RP)cor         75.771%\n",
            "Gmag, (G-RP)cor                         75.673%\n",
            "Gmag, RPmag, (G-RP)cor                  75.673%\n",
            "Gmag, BPmag, (G-RP)cor                  75.641%\n",
            "Gmag, Gmagcor, (G-RP)cor                75.604%\n",
            "Gmag, BPmag, RPmag, Gmagcor, (G-RP)cor  75.522%\n",
            "RPmag, (G-RP)cor                        75.424%\n",
            "BPmag, RPmag, Gmagcor, (G-RP)cor        75.269%\n",
            "Gmag, BPmag, RPmag, (G-RP)cor           75.110%\n",
            "Gmagcor                                 74.384%\n",
            "Gmag, BPmag, Gmagcor                    74.114%\n",
            "Gmag, RPmag, Gmagcor                    74.073%\n",
            "RPmag, Gmagcor                          74.004%\n",
            "BPmag, RPmag                            73.865%\n",
            "BPmag, Gmagcor                          73.857%\n",
            "Gmag                                    73.820%\n",
            "Gmag, Gmagcor                           73.767%\n",
            "Gmag, RPmag                             73.641%\n",
            "BPmag, RPmag, Gmagcor                   73.629%\n",
            "Gmag, BPmag, RPmag                      73.514%\n",
            "Gmag, BPmag, RPmag, Gmagcor             73.449%\n",
            "Gmag, BPmag                             73.037%\n",
            "BPmag                                   72.310%\n",
            "RPmag                                   72.196%\n",
            "(G-RP)cor                               63.441%\n",
            "\n",
            "### SVC Results ###\n",
            "RPmag, (G-RP)cor                        82.718%\n",
            "Gmagcor, (G-RP)cor                      82.559%\n",
            "RPmag, Gmagcor                          82.490%\n",
            "Gmag, RPmag, Gmagcor                    82.482%\n",
            "Gmag, RPmag, Gmagcor, (G-RP)cor         82.453%\n",
            "RPmag                                   82.400%\n",
            "Gmag, Gmagcor                           82.363%\n",
            "RPmag, Gmagcor, (G-RP)cor               82.294%\n",
            "Gmagcor                                 82.253%\n",
            "Gmag, RPmag                             82.176%\n",
            "Gmag, RPmag, (G-RP)cor                  82.114%\n",
            "Gmag, BPmag, (G-RP)cor                  82.057%\n",
            "BPmag, RPmag, (G-RP)cor                 81.971%\n",
            "Gmag, BPmag, Gmagcor                    81.967%\n",
            "BPmag, RPmag, Gmagcor                   81.947%\n",
            "Gmag, BPmag, RPmag, (G-RP)cor           81.882%\n",
            "Gmag                                    81.857%\n",
            "Gmag, BPmag                             81.845%\n",
            "BPmag, Gmagcor, (G-RP)cor               81.829%\n",
            "Gmag, BPmag, Gmagcor, (G-RP)cor         81.808%\n",
            "Gmag, Gmagcor, (G-RP)cor                81.800%\n",
            "Gmag, BPmag, RPmag, Gmagcor             81.796%\n",
            "BPmag, Gmagcor                          81.767%\n",
            "Gmag, BPmag, RPmag, Gmagcor, (G-RP)cor  81.739%\n",
            "BPmag, RPmag, Gmagcor, (G-RP)cor        81.739%\n",
            "BPmag, RPmag                            81.620%\n",
            "Gmag, (G-RP)cor                         81.596%\n",
            "BPmag, (G-RP)cor                        81.514%\n",
            "BPmag                                   81.412%\n",
            "Gmag, BPmag, RPmag                      81.327%\n",
            "(G-RP)cor                               71.547%\n",
            "\n",
            "### LogisticRegression Results ###\n",
            "RPmag, Gmagcor                          82.522%\n",
            "Gmagcor, (G-RP)cor                      82.514%\n",
            "RPmag                                   82.473%\n",
            "Gmag, RPmag, Gmagcor                    82.453%\n",
            "Gmag, Gmagcor                           82.420%\n",
            "RPmag, (G-RP)cor                        82.404%\n",
            "Gmagcor                                 82.400%\n",
            "Gmag, BPmag, Gmagcor                    82.269%\n",
            "Gmag, RPmag                             82.224%\n",
            "RPmag, Gmagcor, (G-RP)cor               82.220%\n",
            "Gmag, RPmag, Gmagcor, (G-RP)cor         82.196%\n",
            "Gmag                                    82.147%\n",
            "Gmag, BPmag, (G-RP)cor                  82.094%\n",
            "Gmag, RPmag, (G-RP)cor                  81.955%\n",
            "Gmag, BPmag                             81.943%\n",
            "BPmag, Gmagcor, (G-RP)cor               81.910%\n",
            "Gmag, BPmag, RPmag, Gmagcor             81.910%\n",
            "BPmag, Gmagcor                          81.882%\n",
            "BPmag, RPmag, Gmagcor                   81.878%\n",
            "Gmag, Gmagcor, (G-RP)cor                81.869%\n",
            "BPmag, RPmag, (G-RP)cor                 81.853%\n",
            "Gmag, BPmag, RPmag, (G-RP)cor           81.776%\n",
            "Gmag, (G-RP)cor                         81.743%\n",
            "BPmag, RPmag, Gmagcor, (G-RP)cor        81.698%\n",
            "BPmag                                   81.698%\n",
            "Gmag, BPmag, Gmagcor, (G-RP)cor         81.653%\n",
            "BPmag, RPmag                            81.624%\n",
            "Gmag, BPmag, RPmag, Gmagcor, (G-RP)cor  81.580%\n",
            "BPmag, (G-RP)cor                        81.567%\n",
            "Gmag, BPmag, RPmag                      81.461%\n",
            "(G-RP)cor                               70.208%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Runs the tests using the global parameters from above\n",
        "run_all_tests(g_data, g_features, g_models, g_result_column,\n",
        "              g_result_nss_indicator, g_result_bss_indicator, g_num_iterations,\n",
        "              g_initial_seed)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}